{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"frcnn_test_inception_resnet_v2.ipynb","provenance":[],"collapsed_sections":["WrH5i5mmrDWY","o0bIjlycyR9_","oFvqGs4acGWl","Mf2taA29RFNs","xcOi5MIMVJpU","0fBt9xNFWsKS","WMev3UMadCzJ","rcRlzqZudKkd","3qGAalfJB8zz"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"kauOILHSqXWw","outputId":"251e5215-1a68-49f7-b90d-e621980c0c57","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657050778822,"user_tz":-60,"elapsed":39540,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"metadata":{"id":"UauKNtFRqydu"},"cell_type":"markdown","source":["### Import libs"]},{"metadata":{"id":"qSbY3Amlqpkh","executionInfo":{"status":"ok","timestamp":1657050782710,"user_tz":-60,"elapsed":3894,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","import random\n","import pprint\n","import sys\n","import time\n","import numpy as np\n","from optparse import OptionParser\n","import pickle\n","import math\n","import cv2\n","import copy\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import os\n","\n","from sklearn.metrics import average_precision_score\n","\n","from keras import backend as K\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n","from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n","from tensorflow.keras.layers import Layer, InputSpec\n","from keras.utils import layer_utils\n","from keras.utils.data_utils import get_file\n","from tensorflow.keras.metrics import categorical_crossentropy\n","\n","from keras.models import Model\n","from keras.utils import generic_utils\n","\n","from keras import initializers, regularizers"],"execution_count":2,"outputs":[]},{"metadata":{"id":"WrH5i5mmrDWY"},"cell_type":"markdown","source":["#### Config setting"]},{"metadata":{"id":"DvJm0FFRsyVu","executionInfo":{"status":"ok","timestamp":1657050782710,"user_tz":-60,"elapsed":10,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["class Config:\n","\n","\tdef __init__(self):\n","\n","\t\t# Print the process or not\n","\t\tself.verbose = True\n","\n","\t\t# Name of base network\n","\t\tself.network = 'InceptionResNetV2'\n","\n","\t\t# Setting for data augmentation\n","\t\tself.use_horizontal_flips = False\n","\t\tself.use_vertical_flips = False\n","\t\tself.rot_90 = False\n","\n","\t\t# Anchor box scales\n","    # Note that if im_size is smaller, anchor_box_scales should be scaled\n","    \n","\t\tself.anchor_box_scales = [128, 256, 512]\n","\t  \n","\t\t# Anchor box ratios\n","\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n","\n","\t\t# Size to resize the smallest side of the image\n","\t\t\n","\t\tself.im_size = 640\n","\n","\t\t# image channel-wise mean to subtract\n","\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n","\t\tself.img_scaling_factor = 1.0\n","\n","\t\t# number of ROIs at once\n","\t\tself.num_rois = 4\n","\n","\t\t# stride at the RPN (this depends on the network configuration)\n","\t\tself.rpn_stride = 16\n","\n","\t\tself.balanced_classes = False\n","\n","\t\t# scaling the stdev\n","\t\tself.std_scaling = 4.0\n","\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n","\n","\t\t# overlaps for RPN\n","\t\tself.rpn_min_overlap = 0.3\n","\t\tself.rpn_max_overlap = 0.7\n","\n","\t\t# overlaps for classifier ROIs\n","\t\tself.classifier_min_overlap = 0.1\n","\t\tself.classifier_max_overlap = 0.5\n","\n","\t\t# placeholder for the class mapping, automatically generated by the parser\n","\t\tself.class_mapping = None\n","\n","\t\tself.model_path = None"],"execution_count":3,"outputs":[]},{"metadata":{"id":"o0bIjlycyR9_"},"cell_type":"markdown","source":["#### Parser the data from annotation file"]},{"metadata":{"id":"vc89E9uAydTX","executionInfo":{"status":"ok","timestamp":1657050782711,"user_tz":-60,"elapsed":10,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def get_data(input_path):\n","\n","\tfound_bg = False\n","\tall_imgs = {}\n","\n","\tclasses_count = {}\n","\n","\tclass_mapping = {}\n","\n","\tvisualise = True\n","\n","\ti = 1\n","\t\n","\twith open(input_path,'r') as f:\n","\n","\t\tprint('Parsing annotation files')\n","\n","\t\tfor line in f:\n","\n","\t\t\t# Print process\n","\t\t\tsys.stdout.write('\\r'+'idx=' + str(i))\n","\t\t\ti += 1\n","\n","\t\t\tline_split = line.strip().split(',')\n","\n","\n","\n","\t\t\t(filename,x1,y1,x2,y2,class_name) = line_split\n","\n","\t\t\tif class_name not in classes_count:\n","\t\t\t\tclasses_count[class_name] = 1\n","\t\t\telse:\n","\t\t\t\tclasses_count[class_name] += 1\n","\n","\t\t\tif class_name not in class_mapping:\n","\t\t\t\tif class_name == 'bg' and found_bg == False:\n","\t\t\t\t\tprint('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n","\t\t\t\t\tfound_bg = True\n","\t\t\t\tclass_mapping[class_name] = len(class_mapping)\n","\n","\t\t\tif filename not in all_imgs:\n","\t\t\t\tall_imgs[filename] = {}\n","\t\t\t\t\n","\t\t\t\timg = cv2.imread(filename)\n","\t\t\t\t(rows,cols) = img.shape[:2]\n","\t\t\t\tall_imgs[filename]['filepath'] = filename\n","\t\t\t\tall_imgs[filename]['width'] = cols\n","\t\t\t\tall_imgs[filename]['height'] = rows\n","\t\t\t\tall_imgs[filename]['bboxes'] = []\n","\n","\n","\t\t\tall_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n","\n","\n","\t\tall_data = []\n","\t\tfor key in all_imgs:\n","\t\t\tall_data.append(all_imgs[key])\n","\t\t\n","\t\t# make sure the bg class is last in the list\n","\t\tif found_bg:\n","\t\t\tif class_mapping['bg'] != len(class_mapping) - 1:\n","\t\t\t\tkey_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n","\t\t\t\tval_to_switch = class_mapping['bg']\n","\t\t\t\tclass_mapping['bg'] = len(class_mapping) - 1\n","\t\t\t\tclass_mapping[key_to_switch] = val_to_switch\n","\t\t\n","\t\treturn all_data, classes_count, class_mapping"],"execution_count":4,"outputs":[]},{"metadata":{"id":"oFvqGs4acGWl"},"cell_type":"markdown","source":["#### Define ROI Pooling Convolutional Layer"]},{"metadata":{"id":"6l32Q85kcMpB","executionInfo":{"status":"ok","timestamp":1657050782711,"user_tz":-60,"elapsed":9,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["class RoiPoolingConv(Layer):\n","\n","    def __init__(self, pool_size, num_rois, **kwargs):\n","\n","        self.dim_ordering = K.image_data_format()\n","        self.pool_size = pool_size\n","        self.num_rois = num_rois\n","\n","        super(RoiPoolingConv, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.nb_channels = input_shape[0][3]   \n","\n","    def compute_output_shape(self, input_shape):\n","        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n","\n","    def call(self, x, mask=None):\n","\n","        assert(len(x) == 2)\n","\n","        # x[0] is image with shape (rows, cols, channels)\n","        img = x[0]\n","\n","        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n","        rois = x[1]\n","\n","        input_shape = K.shape(img)\n","\n","        outputs = []\n","\n","        for roi_idx in range(self.num_rois):\n","\n","            x = rois[0, roi_idx, 0]\n","            y = rois[0, roi_idx, 1]\n","            w = rois[0, roi_idx, 2]\n","            h = rois[0, roi_idx, 3]\n","\n","            x = K.cast(x, 'int32')\n","            y = K.cast(y, 'int32')\n","            w = K.cast(w, 'int32')\n","            h = K.cast(h, 'int32')\n","\n","            # Resized roi of the image to pooling size (7x7)\n","            rs =  tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n","            outputs.append(rs)\n","                \n","\n","        final_output = K.concatenate(outputs, axis=0)\n","\n","        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n","\n","        # permute_dimensions is similar to transpose\n","        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n","\n","        return final_output\n","    \n","    \n","    def get_config(self):\n","        config = {'pool_size': self.pool_size,\n","                  'num_rois': self.num_rois}\n","        base_config = super(RoiPoolingConv, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":5,"outputs":[]},{"metadata":{"id":"Mf2taA29RFNs"},"cell_type":"markdown","source":["#### Inception Resnet V2 model"]},{"cell_type":"code","source":["from keras.layers import Input, Dense, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, AveragePooling2D, TimeDistributed, Concatenate, Lambda\n"],"metadata":{"id":"oiotoWBH7zWe","executionInfo":{"status":"ok","timestamp":1657050782711,"user_tz":-60,"elapsed":8,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class FixedBatchNormalization(Layer):\n","\n","    def __init__(self, epsilon=1e-3, axis=-1,\n","                 weights=None, beta_init='zero', gamma_init='one',\n","                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n","\n","        self.supports_masking = True\n","        self.beta_init = initializers.get(beta_init)\n","        self.gamma_init = initializers.get(gamma_init)\n","        self.epsilon = epsilon\n","        self.axis = axis\n","        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n","        self.beta_regularizer = regularizers.get(beta_regularizer)\n","        self.initial_weights = weights\n","        super(FixedBatchNormalization, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.input_spec = [InputSpec(shape=input_shape)]\n","        shape = (input_shape[self.axis],)\n","\n","        self.gamma = self.add_weight(shape,\n","                                     initializer=self.gamma_init,\n","                                     regularizer=self.gamma_regularizer,\n","                                     name='{}_gamma'.format(self.name),\n","                                     trainable=False)\n","        self.beta = self.add_weight(shape,\n","                                    initializer=self.beta_init,\n","                                    regularizer=self.beta_regularizer,\n","                                    name='{}_beta'.format(self.name),\n","                                    trainable=False)\n","        self.running_mean = self.add_weight(shape, initializer='zero',\n","                                            name='{}_running_mean'.format(self.name),\n","                                            trainable=False)\n","        self.running_std = self.add_weight(shape, initializer='one',\n","                                           name='{}_running_std'.format(self.name),\n","                                           trainable=False)\n","\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","\n","        self.built = True\n","\n","    def call(self, x, mask=None):\n","\n","        assert self.built, 'Layer must be built before being called'\n","        input_shape = K.int_shape(x)\n","\n","        reduction_axes = list(range(len(input_shape)))\n","        del reduction_axes[self.axis]\n","        broadcast_shape = [1] * len(input_shape)\n","        broadcast_shape[self.axis] = input_shape[self.axis]\n","\n","        if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n","            x_normed = K.batch_normalization(\n","                x, self.running_mean, self.running_std,\n","                self.beta, self.gamma,\n","                epsilon=self.epsilon)\n","        else:\n","            # need broadcasting\n","            broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n","            broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n","            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n","            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n","            x_normed = K.batch_normalization(\n","                x, broadcast_running_mean, broadcast_running_std,\n","                broadcast_beta, broadcast_gamma,\n","                epsilon=self.epsilon)\n","\n","        return x_normed\n","\n","    def get_config(self):\n","        config = {'epsilon': self.epsilon,\n","                  'axis': self.axis,\n","                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n","                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None}\n","        base_config = super(FixedBatchNormalization, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"metadata":{"id":"st66z8yG71Iy","executionInfo":{"status":"ok","timestamp":1657050782712,"user_tz":-60,"elapsed":8,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def get_img_output_length(width, height):\n","    def get_output_length(input_length):\n","        # filter_sizes = [3, 3, 3, 3, 3, 3, 3]\n","        # strides = [2, 1, 2, 1, 2, 2, 2]\n","        filter_sizes = [3, 3, 3, 3, 3, 3]\n","        strides = [2, 1, 2, 1, 2, 2]\n","\n","        assert len(filter_sizes) == len(strides)\n","\n","        for i in range(len(filter_sizes)):\n","            input_length = (input_length - filter_sizes[i]) // strides[i] + 1\n","\n","        return input_length\n","\n","    return get_output_length(width), get_output_length(height)\n","\n","\n","def conv2d_bn(x,\n","              filters,\n","              kernel_size,\n","              strides=1,\n","              padding='same',\n","              activation='relu',\n","              use_bias=False,\n","              name=None):\n"," \n","    x = Conv2D(filters,\n","               kernel_size,\n","               strides=strides,\n","               padding=padding,\n","               use_bias=use_bias,\n","               name=name)(x)\n","    if not use_bias:\n","        bn_axis = 1 if K.image_data_format() == 'channels_first' else 3\n","        bn_name = None if name is None else name + '_bn'\n","        x = BatchNormalization(axis=bn_axis,\n","                               scale=False,\n","                               name=bn_name)(x)\n","    if activation is not None:\n","        ac_name = None if name is None else name + '_ac'\n","        x = Activation(activation, name=ac_name)(x)\n","    return x\n","\n","\n","def conv2d_bn_td(x,\n","                 filters,\n","                 kernel_size,\n","                 strides=1,\n","                 padding='same',\n","                 activation='relu',\n","                 use_bias=False,\n","                 name=None):\n","  \n","    x = TimeDistributed(Conv2D(filters,\n","                               kernel_size,\n","                               strides=strides,\n","                               padding=padding,\n","                               use_bias=use_bias),\n","                        name=name)(x)\n","    if not use_bias:\n","        bn_axis = 1 if K.image_data_format() == 'channels_first' else 3\n","        bn_name = None if name is None else name + '_bn'\n","        x = TimeDistributed(BatchNormalization(axis=bn_axis,\n","                                               scale=False),\n","                            name=bn_name)(x)\n","    if activation is not None:\n","        ac_name = None if name is None else name + '_ac'\n","        x = Activation(activation, name=ac_name)(x)\n","    return x\n","\n","\n","def inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n","    \n","    block_name = block_type + '_' + str(block_idx)\n","\n","    if block_type == 'block35':\n","        branch_0 = conv2d_bn(x, 32, 1, name=block_name + '_conv1')\n","        branch_1 = conv2d_bn(x, 32, 1, name=block_name + '_conv2')\n","        branch_1 = conv2d_bn(branch_1, 32, 3, name=block_name + '_conv3')\n","        branch_2 = conv2d_bn(x, 32, 1, name=block_name + '_conv4')\n","        branch_2 = conv2d_bn(branch_2, 48, 3, name=block_name + '_conv5')\n","        branch_2 = conv2d_bn(branch_2, 64, 3, name=block_name + '_conv6')\n","        branches = [branch_0, branch_1, branch_2]\n","    elif block_type == 'block17':\n","        branch_0 = conv2d_bn(x, 192, 1, name=block_name + '_conv1')\n","        branch_1 = conv2d_bn(x, 128, 1, name=block_name + '_conv2')\n","        branch_1 = conv2d_bn(branch_1, 160, [1, 7], name=block_name + '_conv3')\n","        branch_1 = conv2d_bn(branch_1, 192, [7, 1], name=block_name + '_conv4')\n","        branches = [branch_0, branch_1]\n","    elif block_type == 'block8':\n","        branch_0 = conv2d_bn(x, 192, 1, name=block_name + '_conv1')\n","        branch_1 = conv2d_bn(x, 192, 1, name=block_name + '_conv2')\n","        branch_1 = conv2d_bn(branch_1, 224, [1, 3], name=block_name + '_conv3')\n","        branch_1 = conv2d_bn(branch_1, 256, [3, 1], name=block_name + '_conv4')\n","        branches = [branch_0, branch_1]\n","    else:\n","        raise ValueError('Unknown Inception-ResNet block type. '\n","                         'Expects \"block35\", \"block17\" or \"block8\", '\n","                         'but got: ' + str(block_type))\n","\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n","    mixed = Concatenate(\n","        axis=channel_axis, name=block_name + '_mixed')(branches)\n","    up = conv2d_bn(mixed,\n","                   K.int_shape(x)[channel_axis],\n","                   1,\n","                   activation=None,\n","                   use_bias=True,\n","                   name=block_name + '_conv')\n","\n","    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n","                 output_shape=K.int_shape(x)[1:],\n","                 arguments={'scale': scale},\n","                 name=block_name)([x, up])\n","    if activation is not None:\n","        x = Activation(activation, name=block_name + '_ac')(x)\n","    return x\n","\n","\n","def inception_resnet_block_td(x, scale, block_type, block_idx, activation='relu'):\n","    \n","    block_name = block_type + '_' + str(block_idx)\n","\n","    if block_type == 'block35':\n","        branch_0 = conv2d_bn_td(x, 32, 1, name=block_name + '_conv1')\n","        branch_1 = conv2d_bn_td(x, 32, 1, name=block_name + '_conv2')\n","        branch_1 = conv2d_bn_td(branch_1, 32, 3, name=block_name + '_conv3')\n","        branch_2 = conv2d_bn_td(x, 32, 1, name=block_name + '_conv4')\n","        branch_2 = conv2d_bn_td(branch_2, 48, 3, name=block_name + '_conv5')\n","        branch_2 = conv2d_bn_td(branch_2, 64, 3, name=block_name + '_conv6')\n","        branches = [branch_0, branch_1, branch_2]\n","    elif block_type == 'block17':\n","        branch_0 = conv2d_bn_td(x, 192, 1, name=block_name + '_conv1')\n","        branch_1 = conv2d_bn_td(x, 128, 1, name=block_name + '_conv2')\n","        branch_1 = conv2d_bn_td(branch_1, 160, [1, 7], name=block_name + '_conv3')\n","        branch_1 = conv2d_bn_td(branch_1, 192, [7, 1], name=block_name + '_conv4')\n","        branches = [branch_0, branch_1]\n","    elif block_type == 'block8':\n","        branch_0 = conv2d_bn_td(x, 192, 1, name=block_name + '_conv1')\n","        branch_1 = conv2d_bn_td(x, 192, 1, name=block_name + '_conv2')\n","        branch_1 = conv2d_bn_td(branch_1, 224, [1, 3], name=block_name + '_conv3')\n","        branch_1 = conv2d_bn_td(branch_1, 256, [3, 1], name=block_name + '_conv4')\n","        branches = [branch_0, branch_1]\n","    else:\n","        raise ValueError('Unknown Inception-ResNet block type. '\n","                         'Expects \"block35\", \"block17\" or \"block8\", '\n","                         'but got: ' + str(block_type))\n","\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else 4\n","    mixed = Concatenate(\n","        axis=channel_axis, name=block_name + '_mixed')(branches)\n","    up = conv2d_bn_td(mixed,\n","                      K.int_shape(x)[channel_axis],\n","                      1,\n","                      activation=None,\n","                      use_bias=True,\n","                      name=block_name + '_conv')\n","\n","    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n","               output_shape=K.int_shape(x)[1:],\n","               arguments={'scale': scale},\n","               name=block_name)([x, up])\n","    if activation is not None:\n","        x = Activation(activation, name=block_name + '_ac')(x)\n","    return x\n","\n","\n","def nn_base(input_tensor=None, trainable=False):\n","\n","    # Determine proper input shape\n","    if K.image_data_format() == 'th':\n","        input_shape = (3, None, None)\n","    else:\n","        input_shape = (None, None, 3)\n","\n","    if input_tensor is None:\n","        img_input = Input(shape=input_shape)\n","    else:\n","        if not K.is_keras_tensor(input_tensor):\n","            img_input = Input(tensor=input_tensor, shape=input_shape)\n","        else:\n","            img_input = input_tensor\n","\n","    if K.image_data_format() == 'tf':\n","        bn_axis = 3\n","    else:\n","        bn_axis = 1\n","\n","    # Stem block: 35 x 35 x 192\n","    x = conv2d_bn(img_input, 32, 3, strides=2, padding='valid', name='Stem_block' + '_conv1')\n","    x = conv2d_bn(x, 32, 3, padding='valid', name='Stem_block' + '_conv2')\n","    x = conv2d_bn(x, 64, 3, name='Stem_block' + '_conv3')\n","    x = MaxPooling2D(3, strides=2)(x)\n","    x = conv2d_bn(x, 80, 1, padding='valid', name='Stem_block' + '_conv4')\n","    x = conv2d_bn(x, 192, 3, padding='valid', name='Stem_block' + '_conv5')\n","    x = MaxPooling2D(3, strides=2)(x)\n","\n","    # Mixed 5b (Inception-A block): 35 x 35 x 320\n","    branch_0 = conv2d_bn(x, 96, 1, name='Inception_A_block' + '_conv1')\n","    branch_1 = conv2d_bn(x, 48, 1, name='Inception_A_block' + '_conv2')\n","    branch_1 = conv2d_bn(branch_1, 64, 5, name='Inception_A_block' + '_conv3')\n","    branch_2 = conv2d_bn(x, 64, 1, name='Inception_A_block' + '_conv4')\n","    branch_2 = conv2d_bn(branch_2, 96, 3, name='Inception_A_block' + '_conv5')\n","    branch_2 = conv2d_bn(branch_2, 96, 3, name='Inception_A_block' + '_conv6')\n","    branch_pool = AveragePooling2D(3, strides=1, padding='same')(x)\n","    branch_pool = conv2d_bn(branch_pool, 64, 1, name='Inception_A_block' + '_conv7')\n","    branches = [branch_0, branch_1, branch_2, branch_pool]\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n","    x = Concatenate(axis=channel_axis, name='mixed_5b')(branches)\n","\n","    # 10x block35 (Inception-ResNet-A block): 35 x 35 x 320\n","    for block_idx in range(1, 11):\n","        x = inception_resnet_block(x,\n","                                   scale=0.17,\n","                                   block_type='block35',\n","                                   block_idx=block_idx)\n","\n","    # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n","    branch_0 = conv2d_bn(x, 384, 3, strides=2, padding='valid', name='Reduction_A_block' + '_conv1')\n","    branch_1 = conv2d_bn(x, 256, 1, name='Reduction_A_block' + '_conv2')\n","    branch_1 = conv2d_bn(branch_1, 256, 3, name='Reduction_A_block' + '_conv3')\n","    branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding='valid', name='Reduction_A_block' + '_conv4')\n","    branch_pool = MaxPooling2D(3, strides=2, padding='valid')(x)\n","    branches = [branch_0, branch_1, branch_pool]\n","    x = Concatenate(axis=channel_axis, name='mixed_6a')(branches)\n","\n","    # 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n","    for block_idx in range(1, 21):\n","        x = inception_resnet_block(x,\n","                                   scale=0.1,\n","                                   block_type='block17',\n","                                   block_idx=block_idx)\n","\n","    return x\n","\n","\n","def classifier_layers(x, input_shape, trainable=False):\n","\n","    \n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else 4\n","\n","    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n","    branch_0 = conv2d_bn_td(x, 256, 1, name='Reduction_B_block' + '_conv1')\n","    branch_0 = conv2d_bn_td(branch_0, 384, 3, strides=2, padding='valid', name='Reduction_B_block' + '_conv2')\n","    branch_1 = conv2d_bn_td(x, 256, 1, name='Reduction_B_block' + '_conv3')\n","    branch_1 = conv2d_bn_td(branch_1, 288, 3, strides=2, padding='valid', name='Reduction_B_block' + '_conv4')\n","    branch_2 = conv2d_bn_td(x, 256, 1, name='Reduction_B_block' + '_conv5')\n","    branch_2 = conv2d_bn_td(branch_2, 288, 3, name='Reduction_B_block' + '_conv6')\n","    branch_2 = conv2d_bn_td(branch_2, 320, 3, strides=2, padding='valid', name='Reduction_B_block' + '_conv7')\n","    branch_pool = TimeDistributed(MaxPooling2D(3, strides=2, padding='valid'))(x)\n","    branches = [branch_0, branch_1, branch_2, branch_pool]\n","    x = Concatenate(axis=channel_axis, name='mixed_7a')(branches)\n","\n","    # 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n","    for block_idx in range(1, 10):\n","        x = inception_resnet_block_td(x,\n","                                      scale=0.2,\n","                                      block_type='block8',\n","                                      block_idx=block_idx)\n","    x = inception_resnet_block_td(x,\n","                                  scale=1.,\n","                                  activation=None,\n","                                  block_type='block8',\n","                                  block_idx=10)\n","\n","    # Final convolution block: 8 x 8 x 1536\n","    x = conv2d_bn_td(x, 1536, 1, name='conv_7b')\n","\n","    TimeDistributed(GlobalAveragePooling2D(), name='avg_pool')(x)\n","\n","    return x"],"metadata":{"id":"9Dt94F-g73G0","executionInfo":{"status":"ok","timestamp":1657050783434,"user_tz":-60,"elapsed":729,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"execution_count":8,"outputs":[]},{"metadata":{"id":"xcOi5MIMVJpU"},"cell_type":"markdown","source":["####  RPN layer"]},{"metadata":{"id":"gsuV21vpRczQ","executionInfo":{"status":"ok","timestamp":1657050783434,"user_tz":-60,"elapsed":6,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def rpn_layer(base_layers, num_anchors):\n","    \n","    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n","\n","    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n","    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n","\n","    return [x_class, x_regr, base_layers]"],"execution_count":9,"outputs":[]},{"metadata":{"id":"0fBt9xNFWsKS"},"cell_type":"markdown","source":["####  Classifier layer"]},{"metadata":{"id":"0PKSPLRLWwMz","executionInfo":{"status":"ok","timestamp":1657050783434,"user_tz":-60,"elapsed":4,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def classifier_layer(base_layers, input_rois, num_rois, nb_classes=21, trainable=False):\n","\n","    \n","    input_shape = (num_rois, 14, 14, 1088)\n","\n","    pooling_regions = 14\n","\n","    \n","    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n","    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)\n","\n","    out = TimeDistributed(Flatten())(out)\n","\n","    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n","    # note: no regression target for bg class\n","    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n","    return [out_class, out_regr]"],"execution_count":10,"outputs":[]},{"metadata":{"id":"WMev3UMadCzJ"},"cell_type":"markdown","source":["#### Calculate IoU (Intersection of Union)"]},{"metadata":{"id":"Jy5iIBYgdCJD","executionInfo":{"status":"ok","timestamp":1657050783990,"user_tz":-60,"elapsed":560,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def union(au, bu, area_intersection):\n","\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n","\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n","\tarea_union = area_a + area_b - area_intersection\n","\treturn area_union\n","\n","\n","def intersection(ai, bi):\n","\tx = max(ai[0], bi[0])\n","\ty = max(ai[1], bi[1])\n","\tw = min(ai[2], bi[2]) - x\n","\th = min(ai[3], bi[3]) - y\n","\tif w < 0 or h < 0:\n","\t\treturn 0\n","\treturn w*h\n","\n","\n","def iou(a, b):\n","\t# a and b should be (x1,y1,x2,y2)\n","\n","\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n","\t\treturn 0.0\n","\n","\tarea_i = intersection(a, b)\n","\tarea_u = union(a, b, area_i)\n","\n","\treturn float(area_i) / float(area_u + 1e-6)"],"execution_count":11,"outputs":[]},{"metadata":{"id":"rcRlzqZudKkd"},"cell_type":"markdown","source":["#### Calculate the rpn for all anchors of all images"]},{"metadata":{"id":"daPsCZtrdK3S","executionInfo":{"status":"ok","timestamp":1657050783991,"user_tz":-60,"elapsed":13,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n","\t\n","\tdownscale = float(C.rpn_stride) \n","\tanchor_sizes = C.anchor_box_scales   # 128, 256, 512\n","\tanchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n","\tnum_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n","\n","\t# calculate the output map size based on the network architecture\n","\t(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n","\n","\tn_anchratios = len(anchor_ratios)    # 3\n","\t\n","\t# initialise empty output objectives\n","\ty_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n","\ty_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n","\ty_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n","\n","\tnum_bboxes = len(img_data['bboxes'])\n","\n","\tnum_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n","\tbest_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n","\tbest_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n","\tbest_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n","\tbest_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n","\n","\t# get the GT box coordinates, and resize to account for image resizing\n","\tgta = np.zeros((num_bboxes, 4))\n","\tfor bbox_num, bbox in enumerate(img_data['bboxes']):\n","\t\t# get the GT box coordinates, and resize to account for image resizing\n","\t\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n","\t\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n","\t\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n","\t\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n","\t\n","\t# rpn ground truth\n","\n","\tfor anchor_size_idx in range(len(anchor_sizes)):\n","\t\tfor anchor_ratio_idx in range(n_anchratios):\n","\t\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n","\t\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n","\t\t\t\n","\t\t\tfor ix in range(output_width):\t\t\t\t\t\n","\t\t\t\t# x-coordinates of the current anchor box\t\n","\t\t\t\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n","\t\t\t\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n","\t\t\t\t\n","\t\t\t\t# ignore boxes that go across image boundaries\t\t\t\t\t\n","\t\t\t\tif x1_anc < 0 or x2_anc > resized_width:\n","\t\t\t\t\tcontinue\n","\t\t\t\t\t\n","\t\t\t\tfor jy in range(output_height):\n","\n","\t\t\t\t\t# y-coordinates of the current anchor box\n","\t\t\t\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n","\t\t\t\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n","\n","\t\t\t\t\t# ignore boxes that go across image boundaries\n","\t\t\t\t\tif y1_anc < 0 or y2_anc > resized_height:\n","\t\t\t\t\t\tcontinue\n","\n","\t\t\t\t\t# bbox_type indicates whether an anchor should be a target\n","\t\t\t\t\t# Initialize with 'negative'\n","\t\t\t\t\tbbox_type = 'neg'\n","\n","\t\t\t\t\t# this is the best IOU for the (x,y) coord and the current anchor\n","\t\t\t\t\t# note that this is different from the best IOU for a GT bbox\n","\t\t\t\t\tbest_iou_for_loc = 0.0\n","\n","\t\t\t\t\tfor bbox_num in range(num_bboxes):\n","\t\t\t\t\t\t\n","\t\t\t\t\t\t# get IOU of the current GT box and the current anchor box\n","\t\t\t\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n","\t\t\t\t\t\t# calculate the regression targets if they will be needed\n","\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n","\t\t\t\t\t\t\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n","\t\t\t\t\t\t\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n","\t\t\t\t\t\t\tcxa = (x1_anc + x2_anc)/2.0\n","\t\t\t\t\t\t\tcya = (y1_anc + y2_anc)/2.0\n","\n","\t\t\t\t\t\t\ttx = (cx - cxa) / (x2_anc - x1_anc)\n","\t\t\t\t\t\t\tty = (cy - cya) / (y2_anc - y1_anc)\n","\t\t\t\t\t\t\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n","\t\t\t\t\t\t\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tif img_data['bboxes'][bbox_num]['class'] != 'bg':\n","\n","\t\t\t\t\t\t\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n","\t\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num]:\n","\t\t\t\t\t\t\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n","\t\t\t\t\t\t\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n","\t\t\t\t\t\t\t\tbest_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n","\t\t\t\t\t\t\t\tbest_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n","\n","\t\t\t\t\t\t\t# we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n","\t\t\t\t\t\t\tif curr_iou > C.rpn_max_overlap:\n","\t\t\t\t\t\t\t\tbbox_type = 'pos'\n","\t\t\t\t\t\t\t\tnum_anchors_for_bbox[bbox_num] += 1\n","\t\t\t\t\t\t\t\t# we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n","\t\t\t\t\t\t\t\tif curr_iou > best_iou_for_loc:\n","\t\t\t\t\t\t\t\t\tbest_iou_for_loc = curr_iou\n","\t\t\t\t\t\t\t\t\tbest_regr = (tx, ty, tw, th)\n","\n","\t\t\t\t\t\t\t# if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n","\t\t\t\t\t\t\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n","\t\t\t\t\t\t\t\t# gray zone between neg and pos\n","\t\t\t\t\t\t\t\tif bbox_type != 'pos':\n","\t\t\t\t\t\t\t\t\tbbox_type = 'neutral'\n","\n","\t\t\t\t\t# turn on or off outputs depending on IOUs\n","\t\t\t\t\tif bbox_type == 'neg':\n","\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n","\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n","\t\t\t\t\telif bbox_type == 'neutral':\n","\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n","\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n","\t\t\t\t\telif bbox_type == 'pos':\n","\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n","\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n","\t\t\t\t\t\tstart = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n","\t\t\t\t\t\ty_rpn_regr[jy, ix, start:start+4] = best_regr\n","\n","\n","\n","\tfor idx in range(num_anchors_for_bbox.shape[0]):\n","\t\tif num_anchors_for_bbox[idx] == 0:\n","\t\t\t# no box with an IOU greater than zero ...\n","\t\t\tif best_anchor_for_bbox[idx, 0] == -1:\n","\t\t\t\tcontinue\n","\t\t\ty_is_box_valid[\n","\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n","\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n","\t\t\ty_rpn_overlap[\n","\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n","\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n","\t\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n","\t\t\ty_rpn_regr[\n","\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n","\n","\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n","\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n","\n","\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n","\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n","\n","\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n","\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n","\n","\tpos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n","\tneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n","\n","\tnum_pos = len(pos_locs[0])\n","\n","\tnum_regions = 256\n","\n","\tif len(pos_locs[0]) > num_regions/2:\n","\t\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n","\t\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n","\t\tnum_pos = num_regions/2\n","\n","\tif len(neg_locs[0]) + num_pos > num_regions:\n","\t\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n","\t\ty_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n","\n","\ty_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n","\ty_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n","\n","\treturn np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"],"execution_count":12,"outputs":[]},{"metadata":{"id":"3qGAalfJB8zz"},"cell_type":"markdown","source":["#### Get new image size and augment the image"]},{"metadata":{"id":"HKhSFbmB2RTo","executionInfo":{"status":"ok","timestamp":1657050783992,"user_tz":-60,"elapsed":11,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def get_new_img_size(width, height, img_min_side=300):\n","\tif width <= height:\n","\t\tf = float(img_min_side) / width\n","\t\tresized_height = int(f * height)\n","\t\tresized_width = img_min_side\n","\telse:\n","\t\tf = float(img_min_side) / height\n","\t\tresized_width = int(f * width)\n","\t\tresized_height = img_min_side\n","\n","\treturn resized_width, resized_height\n","\n","def augment(img_data, config, augment=True):\n","\tassert 'filepath' in img_data\n","\tassert 'bboxes' in img_data\n","\tassert 'width' in img_data\n","\tassert 'height' in img_data\n","\n","\timg_data_aug = copy.deepcopy(img_data)\n","\n","\timg = cv2.imread(img_data_aug['filepath'])\n","\n","\tif augment:\n","\t\trows, cols = img.shape[:2]\n","\n","\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n","\t\t\timg = cv2.flip(img, 1)\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\tx1 = bbox['x1']\n","\t\t\t\tx2 = bbox['x2']\n","\t\t\t\tbbox['x2'] = cols - x1\n","\t\t\t\tbbox['x1'] = cols - x2\n","\n","\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n","\t\t\timg = cv2.flip(img, 0)\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\ty1 = bbox['y1']\n","\t\t\t\ty2 = bbox['y2']\n","\t\t\t\tbbox['y2'] = rows - y1\n","\t\t\t\tbbox['y1'] = rows - y2\n","\n","\t\tif config.rot_90:\n","\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n","\t\t\tif angle == 270:\n","\t\t\t\timg = np.transpose(img, (1,0,2))\n","\t\t\t\timg = cv2.flip(img, 0)\n","\t\t\telif angle == 180:\n","\t\t\t\timg = cv2.flip(img, -1)\n","\t\t\telif angle == 90:\n","\t\t\t\timg = np.transpose(img, (1,0,2))\n","\t\t\t\timg = cv2.flip(img, 1)\n","\t\t\telif angle == 0:\n","\t\t\t\tpass\n","\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\tx1 = bbox['x1']\n","\t\t\t\tx2 = bbox['x2']\n","\t\t\t\ty1 = bbox['y1']\n","\t\t\t\ty2 = bbox['y2']\n","\t\t\t\tif angle == 270:\n","\t\t\t\t\tbbox['x1'] = y1\n","\t\t\t\t\tbbox['x2'] = y2\n","\t\t\t\t\tbbox['y1'] = cols - x2\n","\t\t\t\t\tbbox['y2'] = cols - x1\n","\t\t\t\telif angle == 180:\n","\t\t\t\t\tbbox['x2'] = cols - x1\n","\t\t\t\t\tbbox['x1'] = cols - x2\n","\t\t\t\t\tbbox['y2'] = rows - y1\n","\t\t\t\t\tbbox['y1'] = rows - y2\n","\t\t\t\telif angle == 90:\n","\t\t\t\t\tbbox['x1'] = rows - y2\n","\t\t\t\t\tbbox['x2'] = rows - y1\n","\t\t\t\t\tbbox['y1'] = x1\n","\t\t\t\t\tbbox['y2'] = x2        \n","\t\t\t\telif angle == 0:\n","\t\t\t\t\tpass\n","\n","\timg_data_aug['width'] = img.shape[1]\n","\timg_data_aug['height'] = img.shape[0]\n","\treturn img_data_aug, img"],"execution_count":13,"outputs":[]},{"metadata":{"id":"0712o8CXkyh1"},"cell_type":"markdown","source":["#### Generate the ground_truth anchors"]},{"metadata":{"id":"TvsEv3RIk0cF","executionInfo":{"status":"ok","timestamp":1657050784276,"user_tz":-60,"elapsed":293,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n","\n","\twhile True:\n","\n","\t\tfor img_data in all_img_data:\n","\t\t\ttry:\n","\n","\t\t\t\t# read in image, and optionally add augmentation\n","\n","\t\t\t\tif mode == 'train':\n","\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=True)\n","\t\t\t\telse:\n","\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=False)\n","\n","\t\t\t\t(width, height) = (img_data_aug['width'], img_data_aug['height'])\n","\t\t\t\t(rows, cols, _) = x_img.shape\n","\n","\t\t\t\tassert cols == width\n","\t\t\t\tassert rows == height\n","\n","\t\t\t\t# get image dimensions for resizing\n","\t\t\t\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n","\n","\t\t\t\t# resize the image so that smalles side is length = 300px\n","\t\t\t\tx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n","\t\t\t\tdebug_img = x_img.copy()\n","\n","\t\t\t\ttry:\n","\t\t\t\t\ty_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n","\t\t\t\texcept:\n","\t\t\t\t\tcontinue\n","\n","\t\t\t\t# Zero-center by mean pixel, and preprocess image\n","\n","\t\t\t\tx_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n","\t\t\t\tx_img = x_img.astype(np.float32)\n","\t\t\t\tx_img[:, :, 0] -= C.img_channel_mean[0]\n","\t\t\t\tx_img[:, :, 1] -= C.img_channel_mean[1]\n","\t\t\t\tx_img[:, :, 2] -= C.img_channel_mean[2]\n","\t\t\t\tx_img /= C.img_scaling_factor\n","\n","\t\t\t\tx_img = np.transpose(x_img, (2, 0, 1))\n","\t\t\t\tx_img = np.expand_dims(x_img, axis=0)\n","\n","\t\t\t\ty_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling\n","\n","\t\t\t\tx_img = np.transpose(x_img, (0, 2, 3, 1))\n","\t\t\t\ty_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n","\t\t\t\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n","\n","\t\t\t\tyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n","\n","\t\t\texcept Exception as e:\n","\t\t\t\tprint(e)\n","\t\t\t\tcontinue"],"execution_count":14,"outputs":[]},{"metadata":{"id":"5cX0N4VDl4zS","executionInfo":{"status":"ok","timestamp":1657050784277,"user_tz":-60,"elapsed":5,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n","\n","\tif len(boxes) == 0:\n","\t\treturn []\n","\n","\t# grab the coordinates of the bounding boxes\n","\tx1 = boxes[:, 0]\n","\ty1 = boxes[:, 1]\n","\tx2 = boxes[:, 2]\n","\ty2 = boxes[:, 3]\n","\n","\tnp.testing.assert_array_less(x1, x2)\n","\tnp.testing.assert_array_less(y1, y2)\n","\n","\t# if the bounding boxes integers, convert them to floats --\n","\t# this is important since we'll be doing a bunch of divisions\n","\tif boxes.dtype.kind == \"i\":\n","\t\tboxes = boxes.astype(\"float\")\n","\n","\t# initialize the list of picked indexes\t\n","\tpick = []\n","\n","\t# calculate the areas\n","\tarea = (x2 - x1) * (y2 - y1)\n","\n","\t# sort the bounding boxes \n","\tidxs = np.argsort(probs)\n","\n","\t# keep looping while some indexes still remain in the indexes\n","\t# list\n","\twhile len(idxs) > 0:\n","\t\t# grab the last index in the indexes list and add the\n","\t\t# index value to the list of picked indexes\n","\t\tlast = len(idxs) - 1\n","\t\ti = idxs[last]\n","\t\tpick.append(i)\n","\n","\t\t# find the intersection\n","\n","\t\txx1_int = np.maximum(x1[i], x1[idxs[:last]])\n","\t\tyy1_int = np.maximum(y1[i], y1[idxs[:last]])\n","\t\txx2_int = np.minimum(x2[i], x2[idxs[:last]])\n","\t\tyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n","\n","\t\tww_int = np.maximum(0, xx2_int - xx1_int)\n","\t\thh_int = np.maximum(0, yy2_int - yy1_int)\n","\n","\t\tarea_int = ww_int * hh_int\n","\n","\t\t# find the union\n","\t\tarea_union = area[i] + area[idxs[:last]] - area_int\n","\n","\t\t# compute the ratio of overlap\n","\t\toverlap = area_int/(area_union + 1e-6)\n","\n","\t\t# delete all indexes from the index list that have\n","\t\tidxs = np.delete(idxs, np.concatenate(([last],\n","\t\t\tnp.where(overlap > overlap_thresh)[0])))\n","\n","\t\tif len(pick) >= max_boxes:\n","\t\t\tbreak\n","\n","\t# return only the bounding boxes that were picked using the integer data type\n","\tboxes = boxes[pick].astype(\"int\")\n","\tprobs = probs[pick]\n","\treturn boxes, probs\n","\n","def apply_regr_np(X, T):\n","\n","\ttry:\n","\t\tx = X[0, :, :]\n","\t\ty = X[1, :, :]\n","\t\tw = X[2, :, :]\n","\t\th = X[3, :, :]\n","\n","\t\ttx = T[0, :, :]\n","\t\tty = T[1, :, :]\n","\t\ttw = T[2, :, :]\n","\t\tth = T[3, :, :]\n","\n","\t\tcx = x + w/2.\n","\t\tcy = y + h/2.\n","\t\tcx1 = tx * w + cx\n","\t\tcy1 = ty * h + cy\n","\n","\t\tw1 = np.exp(tw.astype(np.float64)) * w\n","\t\th1 = np.exp(th.astype(np.float64)) * h\n","\t\tx1 = cx1 - w1/2.\n","\t\ty1 = cy1 - h1/2.\n","\n","\t\tx1 = np.round(x1)\n","\t\ty1 = np.round(y1)\n","\t\tw1 = np.round(w1)\n","\t\th1 = np.round(h1)\n","\t\treturn np.stack([x1, y1, w1, h1])\n","\texcept Exception as e:\n","\t\tprint(e)\n","\t\treturn X\n","    \n","def apply_regr(x, y, w, h, tx, ty, tw, th):\n","    # Apply regression to x, y, w and h\n","\ttry:\n","\t\tcx = x + w/2.\n","\t\tcy = y + h/2.\n","\t\tcx1 = tx * w + cx\n","\t\tcy1 = ty * h + cy\n","\t\tw1 = math.exp(tw) * w\n","\t\th1 = math.exp(th) * h\n","\t\tx1 = cx1 - w1/2.\n","\t\ty1 = cy1 - h1/2.\n","\t\tx1 = int(round(x1))\n","\t\ty1 = int(round(y1))\n","\t\tw1 = int(round(w1))\n","\t\th1 = int(round(h1))\n","\n","\t\treturn x1, y1, w1, h1\n","\n","\texcept ValueError:\n","\t\treturn x, y, w, h\n","\texcept OverflowError:\n","\t\treturn x, y, w, h\n","\texcept Exception as e:\n","\t\tprint(e)\n","\t\treturn x, y, w, h"],"execution_count":15,"outputs":[]},{"metadata":{"id":"vT6X-fqJ1RSl","executionInfo":{"status":"ok","timestamp":1657050784277,"user_tz":-60,"elapsed":4,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n","\t\n","\tregr_layer = regr_layer / C.std_scaling\n","\n","\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n","\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n","\n","\tassert rpn_layer.shape[0] == 1\n","\n","\t(rows, cols) = rpn_layer.shape[1:3]\n","\n","\tcurr_layer = 0\n","\n","\n","\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n","\n","\tfor anchor_size in anchor_sizes:\n","\t\tfor anchor_ratio in anchor_ratios:\n","\n","\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n","\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n","\t\t\t\n","\t\t\t# curr_layer: 0~8 (9 anchors)\n","\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n","\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n","\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n","\n","\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n","\n","\t\t\t# Calculate anchor position and size for each feature map point\n","\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n","\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n","\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n","\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n","\n","\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n","\t\t\tif use_regr:\n","\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n","\n","\t\t\t# Avoid width and height exceeding 1\n","\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n","\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n","\n","\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n","\t\t\t# x1, y1 is top left coordinate\n","\t\t\t# x2, y2 is bottom right coordinate\n","\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n","\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n","\n","\t\t\t# Avoid bboxes drawn outside the feature map\n","\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n","\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n","\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n","\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n","\n","\t\t\tcurr_layer += 1\n","\n","\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n","\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n","\n","\tx1 = all_boxes[:, 0]\n","\ty1 = all_boxes[:, 1]\n","\tx2 = all_boxes[:, 2]\n","\ty2 = all_boxes[:, 3]\n","\n","\t# Find out the bboxes which is illegal and delete them from bboxes list\n","\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n","\n","\tall_boxes = np.delete(all_boxes, idxs, 0)\n","\tall_probs = np.delete(all_probs, idxs, 0)\n","\n","\t# Apply non_max_suppression\n","\t# Only extract the bboxes. Don't need rpn probs in the later process\n","\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n","\n","\treturn result"],"execution_count":16,"outputs":[]},{"metadata":{"id":"Kk14GTaNmqoo"},"cell_type":"markdown","source":["####  Start\n","\n","---\n","\n"]},{"metadata":{"id":"C66bqGuOq7w6","executionInfo":{"status":"ok","timestamp":1657050784278,"user_tz":-60,"elapsed":5,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["base_path = 'drive/My Drive/Faster_percevejo'\n","\n","test_path = 'drive/My Drive/Faster_percevejo/test_class.txt' # Test data (annotation file)\n","\n","test_base_path = 'drive/My Drive/Faster_percevejo/test' # Directory to save the test images\n","\n","config_output_filename = os.path.join(base_path, 'model_InceptionResnetV2_config.pickle')"],"execution_count":17,"outputs":[]},{"metadata":{"id":"Hr7saGnTxC0S","executionInfo":{"status":"ok","timestamp":1657050784579,"user_tz":-60,"elapsed":305,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["with open(config_output_filename, 'rb') as f_in:\n","\tC = pickle.load(f_in)\n","\n","# turn off any data augmentation at test time\n","C.use_horizontal_flips = False\n","C.use_vertical_flips = False\n","C.rot_90 = False"],"execution_count":18,"outputs":[]},{"metadata":{"id":"Kt-1Grs90oD3"},"cell_type":"code","source":["# Load the records\n","record_df = pd.read_csv(C.record_path)\n","\n","r_epochs = len(record_df)\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')\n","plt.title('mean_overlapping_bboxes')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')\n","plt.title('class_acc')\n","\n","plt.show()\n","\n","plt.figure(figsize=(15,5))\n","\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')\n","plt.title('loss_rpn_cls')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')\n","plt.title('loss_rpn_regr')\n","plt.show()\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n","plt.title('loss_class_cls')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')\n","plt.title('loss_class_regr')\n","plt.show()\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n","plt.title('total_loss')\n","\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')\n","plt.title('elapsed_time')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"SJTc51uyFrKc"},"cell_type":"markdown","source":["# Test"]},{"metadata":{"id":"U1J6MchWFjBX","executionInfo":{"status":"ok","timestamp":1657050786600,"user_tz":-60,"elapsed":9,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def format_img_size(img, C):\n","\t\"\"\" formats the image size based on config \"\"\"\n","\timg_min_side = float(C.im_size)\n","\t(height,width,_) = img.shape\n","\t\t\n","\tif width <= height:\n","\t\tratio = img_min_side/width\n","\t\tnew_height = int(ratio * height)\n","\t\tnew_width = int(img_min_side)\n","\telse:\n","\t\tratio = img_min_side/height\n","\t\tnew_width = int(ratio * width)\n","\t\tnew_height = int(img_min_side)\n","\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n","\treturn img, ratio\t\n","\n","def format_img_channels(img, C):\n","\t\"\"\" formats the image channels based on config \"\"\"\n","\timg = img[:, :, (2, 1, 0)]\n","\timg = img.astype(np.float32)\n","\timg[:, :, 0] -= C.img_channel_mean[0]\n","\timg[:, :, 1] -= C.img_channel_mean[1]\n","\timg[:, :, 2] -= C.img_channel_mean[2]\n","\timg /= C.img_scaling_factor\n","\timg = np.transpose(img, (2, 0, 1))\n","\timg = np.expand_dims(img, axis=0)\n","\treturn img\n","\n","def format_img(img, C):\n","\t\"\"\" formats an image for model prediction based on config \"\"\"\n","\timg, ratio = format_img_size(img, C)\n","\timg = format_img_channels(img, C)\n","\treturn img, ratio\n","\n","# Method to transform the coordinates of the bounding box to its original size\n","def get_real_coordinates(ratio, x1, y1, x2, y2):\n","\n","\treal_x1 = int(round(x1 // ratio))\n","\treal_y1 = int(round(y1 // ratio))\n","\treal_x2 = int(round(x2 // ratio))\n","\treal_y2 = int(round(y2 // ratio))\n","\n","\treturn (real_x1, real_y1, real_x2 ,real_y2)"],"execution_count":20,"outputs":[]},{"metadata":{"id":"yLrF_sMQIRPR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657050802311,"user_tz":-60,"elapsed":15719,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}},"outputId":"5e60f9f4-d165-4063-a2be-ac8e6821962b"},"cell_type":"code","source":["num_features = 1088\n","\n","input_shape_img = (None, None, 3)\n","input_shape_features = (None, None, num_features)\n","\n","img_input = Input(shape=input_shape_img)\n","roi_input = Input(shape=(C.num_rois, 4))\n","feature_map_input = Input(shape=input_shape_features)\n","\n","# define the base network (VGG here, can be Resnet50, Inception, etc)\n","shared_layers = nn_base(img_input, trainable=True)\n","\n","# define the RPN, built on the base layers\n","num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n","rpn_layers = rpn_layer(shared_layers, num_anchors)\n","\n","classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))\n","\n","model_rpn = Model(img_input, rpn_layers)\n","model_classifier_only = Model([feature_map_input, roi_input], classifier)\n","\n","model_classifier = Model([feature_map_input, roi_input], classifier)\n","\n","print('Loading weights from {}'.format(C.model_path))\n","model_rpn.load_weights(C.model_path, by_name=True)\n","model_classifier.load_weights(C.model_path, by_name=True)\n","\n","model_rpn.compile(optimizer='sgd', loss='mse')\n","model_classifier.compile(optimizer='sgd', loss='mse')"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading weights from drive/My Drive/Faster_percevejo/model_frcnn_InceptionResnetV2.hdf5\n"]}]},{"metadata":{"id":"oMGXOYQbK_Rx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657050802311,"user_tz":-60,"elapsed":5,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}},"outputId":"ce8a61f4-9178-46ec-be1c-5c462b32def2"},"cell_type":"code","source":["# Switch key value for class mapping\n","class_mapping = C.class_mapping\n","class_mapping = {v: k for k, v in class_mapping.items()}\n","print(class_mapping)\n","class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'percevejo', 1: 'bg'}\n"]}]},{"metadata":{"id":"cX5OTM5Ppl0W","executionInfo":{"status":"ok","timestamp":1657050803367,"user_tz":-60,"elapsed":1058,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["test_imgs = os.listdir(test_base_path)\n","\n","imgs_path = []\n","for i in range(12):\n","\tidx = np.random.randint(len(test_imgs))\n","\timgs_path.append(test_imgs[idx])\n","\n","all_imgs = []\n","\n","classes = {}"],"execution_count":23,"outputs":[]},{"metadata":{"id":"lChm3e9WIijw"},"cell_type":"code","source":["# If the box classification value is less than this, we ignore this box\n","bbox_threshold = 0.5\n","\n","for idx, img_name in enumerate(imgs_path):\n","    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n","        continue\n","    print(img_name)\n","    st = time.time()\n","    filepath = os.path.join(test_base_path, img_name)\n","\n","    img = cv2.imread(filepath)\n","\n","    X, ratio = format_img(img, C)\n","    \n","    X = np.transpose(X, (0, 2, 3, 1))\n","\n","    # get output layer Y1, Y2 from the RPN and the feature maps F\n","    # Y1: y_rpn_cls\n","    # Y2: y_rpn_regr\n","    [Y1, Y2, F] = model_rpn.predict(X)\n","\n","    # Get bboxes by applying NMS \n","    # R.shape = (300, 4)\n","    R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.7)\n","\n","    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n","    R[:, 2] -= R[:, 0]\n","    R[:, 3] -= R[:, 1]\n","\n","    # apply the spatial pyramid pooling to the proposed regions\n","    bboxes = {}\n","    probs = {}\n","\n","    for jk in range(R.shape[0]//C.num_rois + 1):\n","        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n","        if ROIs.shape[1] == 0:\n","            break\n","\n","        if jk == R.shape[0]//C.num_rois:\n","            #pad R\n","            curr_shape = ROIs.shape\n","            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n","            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n","            ROIs_padded[:, :curr_shape[1], :] = ROIs\n","            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n","            ROIs = ROIs_padded\n","\n","        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n","\n","        # Calculate bboxes coordinates on resized image\n","        for ii in range(P_cls.shape[1]):\n","            # Ignore 'bg' class\n","            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n","                continue\n","\n","            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n","\n","            if cls_name not in bboxes:\n","                bboxes[cls_name] = []\n","                probs[cls_name] = []\n","\n","            (x, y, w, h) = ROIs[0, ii, :]\n","\n","            cls_num = np.argmax(P_cls[0, ii, :])\n","            try:\n","                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n","                tx /= C.classifier_regr_std[0]\n","                ty /= C.classifier_regr_std[1]\n","                tw /= C.classifier_regr_std[2]\n","                th /= C.classifier_regr_std[3]\n","                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n","            except:\n","                pass\n","            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n","            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n","\n","    all_dets = []\n","\n","    for key in bboxes:\n","        bbox = np.array(bboxes[key])\n","\n","        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n","\n","        with open('drive/My Drive/Bedbug/result_test/'+img_name+'.txt','w') as f:\n","          for jk in range(new_boxes.shape[0]):\n","              (x1, y1, x2, y2) = new_boxes[jk,:]\n","\n","              # Calculate real coordinates on original image\n","              (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n","\n","              cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n","              a=str('Bedbug')\n","              textLabel = '{}: {}'.format(a,int(100*new_probs[jk]))\n","              all_dets.append((key,100*new_probs[jk]))\n","\n","              (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n","              textOrg = (real_x1, real_y1-0)\n","\n","              cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n","              cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n","              cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n","              #f=open(img_name+'.txt','w')\n","              f.write(str(x1) + ',' + str(y1) + ',' + str(x2) + ',' + str(y2) + ',' + 'percevejo'+'\\n')\n","          f.close()\n","\n","    cv2.putText(img,str(len(all_dets))+'Bedbug',(0,25),cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1)\n","    path='drive/My Drive/Bedbug/result_test/'+img_name\n","    cv2.imwrite(path,img)\n","\n","\n","  \n","    print('Elapsed time = {}'.format(time.time() - st))\n","    print(len(all_dets))\n","    print(all_dets)\n","    plt.figure(figsize=(10,10))\n","    #plt.grid()\n","    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"PbMC1r8BqC9k"},"cell_type":"markdown","source":["#### Measure mAP"]},{"metadata":{"id":"HGt9OlDoU4vM","executionInfo":{"status":"ok","timestamp":1657050803368,"user_tz":-60,"elapsed":5,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def get_map(pred, gt, f):\n","\tT = {}\n","\tP = {}\n","\tfx, fy = f\n","\n","\tfor bbox in gt:\n","\t\tbbox['bbox_matched'] = False\n","\n","\tpred_probs = np.array([s['prob'] for s in pred])\n","\tbox_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]\n","\n","\tfor box_idx in box_idx_sorted_by_prob:\n","\t\tpred_box = pred[box_idx]\n","\t\tpred_class = pred_box['class']\n","\t\tpred_x1 = pred_box['x1']\n","\t\tpred_x2 = pred_box['x2']\n","\t\tpred_y1 = pred_box['y1']\n","\t\tpred_y2 = pred_box['y2']\n","\t\tpred_prob = pred_box['prob']\n","\t\tif pred_class not in P:\n","\t\t\tP[pred_class] = []\n","\t\t\tT[pred_class] = []\n","\t\tP[pred_class].append(pred_prob)\n","\t\tfound_match = False\n","\n","\t\tfor gt_box in gt:\n","\t\t\tgt_class = gt_box['class']\n","\t\t\tgt_x1 = gt_box['x1']/fx\n","\t\t\tgt_x2 = gt_box['x2']/fx\n","\t\t\tgt_y1 = gt_box['y1']/fy\n","\t\t\tgt_y2 = gt_box['y2']/fy\n","\t\t\tgt_seen = gt_box['bbox_matched']\n","\t\t\tif gt_class != pred_class:\n","\t\t\t\tcontinue\n","\t\t\tif gt_seen:\n","\t\t\t\tcontinue\n","\t\t\tiou_map = iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n","\t\t\tif iou_map >= 0.5:\n","\t\t\t\tfound_match = True\n","\t\t\t\tgt_box['bbox_matched'] = True\n","\t\t\t\tbreak\n","\t\t\telse:\n","\t\t\t\tcontinue\n","\n","\t\tT[pred_class].append(int(found_match))\n","\n","\tfor gt_box in gt:\n","\t\tif not gt_box['bbox_matched']:# and not gt_box['difficult']:\n","\t\t\tif gt_box['class'] not in P:\n","\t\t\t\tP[gt_box['class']] = []\n","\t\t\t\tT[gt_box['class']] = []\n","\n","\t\t\tT[gt_box['class']].append(1)\n","\t\t\tP[gt_box['class']].append(0)\n","\n","\t#import pdb\n","\t#pdb.set_trace()\n","\treturn T, P"],"execution_count":24,"outputs":[]},{"metadata":{"id":"7m0J2BiEqLgA","executionInfo":{"status":"ok","timestamp":1657050803368,"user_tz":-60,"elapsed":4,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["def format_img_map(img, C):\n","\n","\n","\timg_min_side = float(C.im_size)\n","\t(height,width,_) = img.shape\n","\t\n","\tif width <= height:\n","\t\tf = img_min_side/width\n","\t\tnew_height = int(f * height)\n","\t\tnew_width = int(img_min_side)\n","\telse:\n","\t\tf = img_min_side/height\n","\t\tnew_width = int(f * width)\n","\t\tnew_height = int(img_min_side)\n","\tfx = width/float(new_width)\n","\tfy = height/float(new_height)\n","\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n","\t# Change image channel from BGR to RGB\n","\timg = img[:, :, (2, 1, 0)]\n","\timg = img.astype(np.float32)\n","\timg[:, :, 0] -= C.img_channel_mean[0]\n","\timg[:, :, 1] -= C.img_channel_mean[1]\n","\timg[:, :, 2] -= C.img_channel_mean[2]\n","\timg /= C.img_scaling_factor\n","\t# Change img shape from (height, width, channel) to (channel, height, width)\n","\timg = np.transpose(img, (2, 0, 1))\n","\t# Expand one dimension at axis 0\n","\t# img shape becames (1, channel, height, width)\n","\timg = np.expand_dims(img, axis=0)\n","\treturn img, fx, fy"],"execution_count":25,"outputs":[]},{"metadata":{"id":"xNUNkonDqOwg"},"cell_type":"code","source":["print(class_mapping)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"63-8osThqSjE"},"cell_type":"code","source":["# This might takes a while to parser the data\n","test_imgs, _, _ = get_data(test_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.seterr(invalid='ignore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCr1GhgcRbJ1","executionInfo":{"status":"ok","timestamp":1657050805256,"user_tz":-60,"elapsed":4,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}},"outputId":"ceb1afc4-57dc-48eb-e3a5-0b81219435b4"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"]},"metadata":{},"execution_count":28}]},{"metadata":{"id":"RfNn-rWQsWPs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d9c8409-eebc-48c8-c7f7-e96fe786cd0f","executionInfo":{"status":"ok","timestamp":1657051591458,"user_tz":-60,"elapsed":144877,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}}},"cell_type":"code","source":["T = {}\n","P = {}\n","mAPs = []\n","for idx, img_data in enumerate(test_imgs):\n","    print('{}/{}'.format(idx,len(test_imgs)))\n","    st = time.time()\n","    filepath = img_data['filepath']\n","\n","    img = cv2.imread(filepath)\n","\n","    X, fx, fy = format_img_map(img, C)\n","\n","    # Change X (img) shape from (1, channel, height, width) to (1, height, width, channel)\n","    X = np.transpose(X, (0, 2, 3, 1))\n","\n","    # get the feature maps and output from the RPN\n","    [Y1, Y2, F] = model_rpn.predict(X)\n","\n","\n","    R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.7)\n","\n","    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n","    R[:, 2] -= R[:, 0]\n","    R[:, 3] -= R[:, 1]\n","\n","    # apply the spatial pyramid pooling to the proposed regions\n","    bboxes = {}\n","    probs = {}\n","\n","    for jk in range(R.shape[0] // C.num_rois + 1):\n","        ROIs = np.expand_dims(R[C.num_rois * jk:C.num_rois * (jk + 1), :], axis=0)\n","        if ROIs.shape[1] == 0:\n","            break\n","\n","        if jk == R.shape[0] // C.num_rois:\n","            # pad R\n","            curr_shape = ROIs.shape\n","            target_shape = (curr_shape[0], C.num_rois, curr_shape[2])\n","            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n","            ROIs_padded[:, :curr_shape[1], :] = ROIs\n","            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n","            ROIs = ROIs_padded\n","\n","        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n","\n","      \n","        for ii in range(P_cls.shape[1]):\n","\n","            # If class name is 'bg', continue\n","            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n","                continue\n","\n","            # Get class name\n","            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n","\n","            if cls_name not in bboxes:\n","                bboxes[cls_name] = []\n","                probs[cls_name] = []\n","\n","            (x, y, w, h) = ROIs[0, ii, :]\n","\n","            cls_num = np.argmax(P_cls[0, ii, :])\n","            try:\n","                (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]\n","                tx /= C.classifier_regr_std[0]\n","                ty /= C.classifier_regr_std[1]\n","                tw /= C.classifier_regr_std[2]\n","                th /= C.classifier_regr_std[3]\n","                x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n","            except:\n","                pass\n","            bboxes[cls_name].append([16 * x, 16 * y, 16 * (x + w), 16 * (y + h)])\n","            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n","\n","    all_dets = []\n","\n","    for key in bboxes:\n","        bbox = np.array(bboxes[key])\n","\n","        # Apply non-max-suppression on final bboxes to get the output bounding boxe\n","        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n","        for jk in range(new_boxes.shape[0]):\n","            (x1, y1, x2, y2) = new_boxes[jk, :]\n","            det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n","            all_dets.append(det)\n","\n","    print('Elapsed time = {}'.format(time.time() - st))\n","    t, p = get_map(all_dets, img_data['bboxes'], (fx, fy))\n","    for key in t.keys():\n","        if key not in T:\n","            T[key] = []\n","            P[key] = []\n","        T[key].extend(t[key])\n","        P[key].extend(p[key])\n","    all_aps = []\n","    for key in T.keys():\n","        ap = average_precision_score(T[key], P[key],pos_label=0)\n","        print('{} AP: {}'.format(key, ap))\n","        all_aps.append(ap)\n","    print('mAP = {}'.format(np.mean(np.array(all_aps))))\n"," \n","    mAPs.append(np.mean(np.array(all_aps)))\n","    #print(T)\n","    #print(P)\n","    \n","print()\n"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["0/5\n","Elapsed time = 43.387468099594116\n","percevejo AP: nan\n","mAP = nan\n","1/5\n","Elapsed time = 26.042036533355713\n","percevejo AP: 1.0\n","mAP = 1.0\n","2/5\n","Elapsed time = 26.415133476257324\n","percevejo AP: 0.7713789682539682\n","mAP = 0.7713789682539682\n","3/5\n","Elapsed time = 22.97730278968811\n","percevejo AP: 0.7713789682539682\n","mAP = 0.7713789682539682\n","4/5\n","Elapsed time = 26.614561319351196\n","percevejo AP: 0.6427579365079366\n","mAP = 0.6427579365079366\n","\n"]}]},{"metadata":{"id":"oVBClYzBZtZV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657051192362,"user_tz":-60,"elapsed":5,"user":{"displayName":"Cláudia Teixeira","userId":"01958981397105673517"}},"outputId":"b7d0254c-de1b-4dc1-df9f-c2139779762c"},"cell_type":"code","source":["print('AP = {}'.format(np.mean(np.array(all_aps))))"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["mAP = 0.6427579365079366\n"]}]}]}